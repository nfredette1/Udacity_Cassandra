{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T05:56:15.021036Z",
     "start_time": "2019-05-04T05:56:14.997694Z"
    },
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "div.text_cell_render h1, h2, h3, h4, h5 { \n",
       "font-family: 'Georgia';\n",
       "}\n",
       "\n",
       "\n",
       "div.text_cell_render { /* Customize text cells */\n",
       "font-family: 'Avenir';\n",
       "font-size:15px;\n",
       "line-height:18px;\n",
       "color: #292929;\n",
       "font-weight:400;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "\n",
    "div.text_cell_render h1, h2, h3, h4, h5 { \n",
    "font-family: 'Georgia';\n",
    "}\n",
    "\n",
    "\n",
    "div.text_cell_render { /* Customize text cells */\n",
    "font-family: 'Avenir';\n",
    "font-size:15px;\n",
    "line-height:18px;\n",
    "color: #292929;\n",
    "font-weight:400;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T05:56:15.467953Z",
     "start_time": "2019-05-04T05:56:15.461174Z"
    },
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Run for formatting display width\n",
    "display(HTML(\"<style>.container { width:100% !important;}</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project 2: Data Modeling with Postgres "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Introduction \n",
    "\n",
    "So, in this project, we have startup, called Sparkify who wants to analyze the data they've been collecting on their new music streaming app regarding: \n",
    "\n",
    "1. songs \n",
    "2. user activity \n",
    "\n",
    "<img src=\"images/sparkify.png\">\n",
    "\n",
    "The analytics team is actually trying to understand what songs, users are listening to. Okay? They want to know which songs are most popular, what is the listening time of the songs, etc.  \n",
    "\n",
    "So, as they are a startup, they don't have an easy way of querying their data.  \n",
    "\n",
    "Their data currently resides in a directory of `CSV` files about user activity on the app. \n",
    "\n",
    "\n",
    "Now, they would like a data engineer to create an Apache Cassandra database designed to optimize their queries concerned with understanding what songs users are listening to. \n",
    "\n",
    "In other words, you have to create a database that is optimized for _song play analysis_.   \n",
    "\n",
    "\n",
    "Our task is to create a database schema and ETL pipeline for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "So, in this project you will apply what you've learned on data modeling with Apache Cassandra and build an ETL pipeline using Python.   \n",
    "\n",
    "Make sure you have completed [Lesson 4: NoSQL Data Models](https://classroom.udacity.com/nanodegrees/nd027/parts/f7dbb125-87a2-4369-bb64-dc5c21bb668a/modules/c0e48224-f2d0-4bf5-ac02-3e1493e530fc/lessons/73fd6e35-3319-4520-94b5-9651437235d7/concepts/864ae5c9-e9fb-47ca-bcde-b152d00543a1). The learnings from this lesson are the only way you will be able to complete this project. \n",
    "\n",
    "To complete this project, you will need to model your data by creating tables in Apache Cassandra to run queries.  \n",
    "\n",
    "\n",
    "Now you may be wondering, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Project Datasets \n",
    "\n",
    "Now, for those of you who want a refresher on `CSV` or flat files, in general, I'll let my friend, [David explain it to you](https://www.youtube.com/watch?v=bLKVRIhrZUY). \n",
    "\n",
    "Basically, they are just comma separated values which represent tabular data in plain text format, with one data record per line. In each line, we have multiple fields  separated using commas. \n",
    "\n",
    "<img src=\"images/flat_file_structure.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can see the header line, `ranking, critic_score, title, number_of_critic_ratings` in the `.csv` text file on the left. And we can see it's the first row in excel file on the right also.  \n",
    "\n",
    "Then, we see the line `1, 99, The Wizard of Oz (1939), 110` on the left and the same values entered in the excel file on the right. So you get the sense of `csv`s work. \n",
    "\n",
    "This is how our `events_data` folder looks like in our workspace. We have 30 files, one for each day in November 2018. Each file contains events or logs from a day. \n",
    "\n",
    "<img src='images/event_data_files.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here is how one of our `events_data`'s `.csv` file looks like: \n",
    "\n",
    "> This is data fro 1st November, 2018. \n",
    "<img src=\"images/csv_file_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can see that we have the header row: \n",
    "\n",
    "```\n",
    "artist,auth,firstName,gender,itemInSession,lastName,length,level,location,method,page,registration,sessionId,song,status,ts,userId\n",
    "``` \n",
    "\n",
    "Then, we have rows of data with a value for each column. \n",
    "\n",
    "Now, since this is event data from the Sparkify app. We have different number of records for each day. Some have just 16 entries, like shows in the `2018-11-01-events.csv` file, while some days have more than 100 events:  \n",
    "\n",
    "> This data is from 5th November 2018. \n",
    "<img src=\"images/csv_file_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Let's read in the first 5 rows of the events from 1st November, 2018. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T17:40:49.780127Z",
     "start_time": "2019-05-03T17:40:48.111407Z"
    },
    "editable": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'event_data/2018-11-01-events.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\n0170199\\Projects\\DE_Projects\\project_2_walkthrough\\LM_project2_walkthrough.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/n0170199/Projects/DE_Projects/project_2_walkthrough/LM_project2_walkthrough.ipynb#ch0000010?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m \n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/n0170199/Projects/DE_Projects/project_2_walkthrough/LM_project2_walkthrough.ipynb#ch0000010?line=1'>2</a>\u001b[0m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mevent_data/2018-11-01-events.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\Users\\n0170199\\Projects\\udacity\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/util/_decorators.py?line=304'>305</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/util/_decorators.py?line=305'>306</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/util/_decorators.py?line=306'>307</a>\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/util/_decorators.py?line=307'>308</a>\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/util/_decorators.py?line=308'>309</a>\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/util/_decorators.py?line=309'>310</a>\u001b[0m     )\n\u001b[1;32m--> <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/util/_decorators.py?line=310'>311</a>\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\n0170199\\Projects\\udacity\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/parsers/readers.py?line=664'>665</a>\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/parsers/readers.py?line=665'>666</a>\u001b[0m     dialect,\n\u001b[0;32m    <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/parsers/readers.py?line=666'>667</a>\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/parsers/readers.py?line=675'>676</a>\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/parsers/readers.py?line=676'>677</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/parsers/readers.py?line=677'>678</a>\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/parsers/readers.py?line=679'>680</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\n0170199\\Projects\\udacity\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/parsers/readers.py?line=571'>572</a>\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/parsers/readers.py?line=573'>574</a>\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/parsers/readers.py?line=574'>575</a>\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/parsers/readers.py?line=576'>577</a>\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/parsers/readers.py?line=577'>578</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\n0170199\\Projects\\udacity\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/parsers/readers.py?line=929'>930</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/parsers/readers.py?line=931'>932</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/parsers/readers.py?line=932'>933</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\n0170199\\Projects\\udacity\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/parsers/readers.py?line=1212'>1213</a>\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/parsers/readers.py?line=1213'>1214</a>\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/parsers/readers.py?line=1214'>1215</a>\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/parsers/readers.py?line=1215'>1216</a>\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/parsers/readers.py?line=1216'>1217</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/parsers/readers.py?line=1217'>1218</a>\u001b[0m     f,\n\u001b[0;32m   <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/parsers/readers.py?line=1218'>1219</a>\u001b[0m     mode,\n\u001b[0;32m   <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/parsers/readers.py?line=1219'>1220</a>\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/parsers/readers.py?line=1220'>1221</a>\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/parsers/readers.py?line=1221'>1222</a>\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/parsers/readers.py?line=1222'>1223</a>\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/parsers/readers.py?line=1223'>1224</a>\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/parsers/readers.py?line=1224'>1225</a>\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/parsers/readers.py?line=1225'>1226</a>\u001b[0m )\n\u001b[0;32m   <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/parsers/readers.py?line=1226'>1227</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/parsers/readers.py?line=1227'>1228</a>\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\n0170199\\Projects\\udacity\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/common.py?line=783'>784</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/common.py?line=784'>785</a>\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/common.py?line=785'>786</a>\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/common.py?line=786'>787</a>\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/common.py?line=787'>788</a>\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/common.py?line=788'>789</a>\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/common.py?line=789'>790</a>\u001b[0m             handle,\n\u001b[0;32m    <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/common.py?line=790'>791</a>\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/common.py?line=791'>792</a>\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/common.py?line=792'>793</a>\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/common.py?line=793'>794</a>\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/common.py?line=794'>795</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/common.py?line=795'>796</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/common.py?line=796'>797</a>\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/n0170199/Projects/udacity/lib/site-packages/pandas/io/common.py?line=797'>798</a>\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'event_data/2018-11-01-events.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "pd.read_csv('event_data/2018-11-01-events.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now, we can see that we have a column for the `artist` name.\n",
    "\n",
    "Looking at the information on the user, we have the columns or `firstName` and `lastName` of the user.    \n",
    "Then, we also have columns telling us the `gender` of the user, the `level` of the app they are using (free or paid), their `location`, their `registration` and `userId`.   \n",
    "\n",
    "Looking at the information regarding the song they are listening to, we have the duration, or `length` of the song and the name of the `song`. \n",
    "\n",
    "Finally, looking at some generic information about the log, we see that we have the `iteminSession` column, the `method`, `page`, `sessionId`, `status` and the timestamp, i.e. `ts`.  \n",
    "\n",
    "__NOTE:__ The `page` column is an important one. It help us filter which logs are related to song plays. That is, the logs with the `page == 'NextSong'` is a log wherein the user was listening to a song. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "So, now, let's get into the Project Template and see which files we are going to be working with. \n",
    "\n",
    "## Project Template \n",
    "\n",
    "For this project, it is advisable to work in the workspace itself, as setting up Cassandra is a little tricky.  \n",
    " \n",
    "\n",
    "The project template includes only one Jupyter Notebook, in which you'll carry out the project steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 📝 My Notes: \n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Project Steps \n",
    "\n",
    "Below are the steps you can follow to complete each component of this project. \n",
    "\n",
    "### Modeling your NoSQL  or Apache Cassandra database \n",
    "\n",
    "__STEP 1__ Run the 4 code cells in Part I. This will import the python packages and create an `event_datafile_new.csv` which will have data from all `.csv`s in `event_data` folder. It will have $6821$ rows.   \n",
    "\n",
    "\n",
    "__STEP 2__ After connecting to the local instance of Apache Cassandra and creating a session, create and set a keyspace for this project.   \n",
    "\n",
    "\n",
    "__STEP 3__ Create queries to ask the given 3 questions from the data. These queries will include:\n",
    "- `CREATE TABLE` queries to create the tables to answer the given question. \n",
    "- `INSERT` queries to populate the created tables with data from `event_datafile_new.csv`. \n",
    "- `SELECT` queries to select the relevant data from the created table to answer the given question. \n",
    "\n",
    "__STEP 4__ `DROP` the Tables you created. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Can we start the project now? \n",
    "\n",
    "\n",
    "## Wait! Before starting the project, make sure to go through the [Project Rubric](https://review.udacity.com/#!/rubrics/2475/view)\n",
    "\n",
    "\n",
    "Now, before we start with the project, I want us to first go through the project rubric. It's always a good idea to go through the project rubric once before we do the project, and once after we do the project.\n",
    "\n",
    "This rubric serves as a project checklist for us. You can see the tasks you need to do and make sure you meet specifications on the project\n",
    "\n",
    "\n",
    "### Basically, here are the requirements: \n",
    "\n",
    "Just read them through. You won't really understand much of it but it's like reading the questions of a reading comprehension before actually reading the essay. \n",
    "\n",
    "\n",
    "### ETL Pipeline Processing \n",
    "\n",
    "- Student creates __`event_data_new.csv`__ file.    \n",
    "\n",
    "- Student uses the __appropriate datatype within the CREATE__ statement.For e.g., `artist_name` and `song_title` use `TEXT`, length use `FLOAT` datatypes.  \n",
    "\n",
    "#### Data Modeling \n",
    "\n",
    "- Student creates the __correct Apache Cassandra tables__ for each of the three queries. The `CREATE TABLE` statement should include the appropriate table. Student should adhere to the __one table per query rule of Apache Cassandra__. The student is allowed to use the same table for two of the questions, where it makes sense.    \n",
    "\n",
    "\n",
    "- Student demonstrates good understanding of data modeling by generating __correct `SELECT` statements to generate the result being asked for in the question__. The SELECT statement should NOT use `ALLOW FILTERING` to generate the results. For e.g., Query 3, `SELECT `statement should not require anything more than user name first and last name in the `SELECT` statement IF the table has been created with the correct COMPOSITE PRIMARY KEY, including partitions and clustering columns.   \n",
    "\n",
    "\n",
    "- Student should use __table names that reflect the query__ and the result it will generate. Table names should include alphanumeric characters and underscores, and table names must start with a letter. We are looking for table names that provide a good general sense of what this query will generate. For e.g., for Query 2, an appropriate table name should reflect song playlist in session (e.g., name could be `song_playlist_session`). Students should not be using table names like `query_1` or `project_1`, etc. as table names need to be descriptive. \n",
    "\n",
    "\n",
    "- The __sequence in which columns appear should reflect how the data is partitioned and the order of the data within the partitions__. The sequence of the columns in the `CREATE` and `INSERT` statements should follow the order of the COMPOSITE PRIMARY KEY and CLUSTERING columns. The data should be inserted and retrieved in the same order as how the COMPOSITE PRIMARY KEY is set up. This is important to the student because Apache Cassandra is a partition row store, which means the partition key determines which any particular row is stored on which node. In case of composite partition key, partitions are distributed across the nodes of the cluster and how they are chunked for write purposes. Any clustering column(s) would determine the order in which the data is sorted within the partition.   \n",
    "\n",
    "#### PRIMARY KEYS\n",
    "\n",
    "- The combination of the PARTITION KEY alone or with the addition of CLUSTERING COLUMNS should be used appropriately to __uniquely identify each row__. For e.g., in Query 3, student should not only use song as PARTITION KEY. Similarly, the student does not need to user both `firstName` and `lastName` along with `userId` for query 3 clustering columns, as `song` and `userId` together will uniquely identify each row. The student should include clustering columns as part of the COMPOSITE PRIMARY KEY and understand that a COMPOSITE PRIMARY KEY uniquely identifies each row.   \n",
    "\n",
    "#### Presentation \n",
    "\n",
    "- The notebooks should include a description of the query the data is modeled after. The student can include headers right above the SELECT statement cell to highlight the responses to the questions.  \n",
    "\n",
    "\n",
    "- Code should be organized well into the different queries. Any in-line comments that were clearly part of the project instructions should be removed so the notebook provides a professional look.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 📝 My Notes: \n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Part I: ETL Pipeline for Pre-Processing the Files \n",
    "\n",
    "### Import Python Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T17:47:49.424776Z",
     "start_time": "2019-05-03T17:47:49.413386Z"
    },
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import python packages\n",
    "import pandas as pd\n",
    "import cassandra \n",
    "import re\n",
    "import os\n",
    "import glob \n",
    "import numpy as np\n",
    "import json \n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Create list of filepaths to process original event csv data files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\n0170199\\Projects\\DE_Projects\\project_2_walkthrough\n"
     ]
    }
   ],
   "source": [
    "# checking your current working directory \n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get absolute path of subfolder called `event_data` \n",
    "filepath = os.getcwd() + '/event_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The following block of code creates a python list called `file_path_list` which has the list of absolute file paths of the $30$ files in `event_data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\n0170199\\\\Projects\\\\DE_Projects\\\\project_2_walkthrough/event_data\\\\2018-11-01-events.csv', 'c:\\\\Users\\\\n0170199\\\\Projects\\\\DE_Projects\\\\project_2_walkthrough/event_data\\\\2018-11-02-events.csv', 'c:\\\\Users\\\\n0170199\\\\Projects\\\\DE_Projects\\\\project_2_walkthrough/event_data\\\\2018-11-03-events.csv', 'c:\\\\Users\\\\n0170199\\\\Projects\\\\DE_Projects\\\\project_2_walkthrough/event_data\\\\2018-11-04-events.csv', 'c:\\\\Users\\\\n0170199\\\\Projects\\\\DE_Projects\\\\project_2_walkthrough/event_data\\\\2018-11-05-events.csv', 'c:\\\\Users\\\\n0170199\\\\Projects\\\\DE_Projects\\\\project_2_walkthrough/event_data\\\\2018-11-06-events.csv', 'c:\\\\Users\\\\n0170199\\\\Projects\\\\DE_Projects\\\\project_2_walkthrough/event_data\\\\2018-11-07-events.csv', 'c:\\\\Users\\\\n0170199\\\\Projects\\\\DE_Projects\\\\project_2_walkthrough/event_data\\\\2018-11-08-events.csv', 'c:\\\\Users\\\\n0170199\\\\Projects\\\\DE_Projects\\\\project_2_walkthrough/event_data\\\\2018-11-09-events.csv', 'c:\\\\Users\\\\n0170199\\\\Projects\\\\DE_Projects\\\\project_2_walkthrough/event_data\\\\2018-11-10-events.csv', 'c:\\\\Users\\\\n0170199\\\\Projects\\\\DE_Projects\\\\project_2_walkthrough/event_data\\\\2018-11-11-events.csv', 'c:\\\\Users\\\\n0170199\\\\Projects\\\\DE_Projects\\\\project_2_walkthrough/event_data\\\\2018-11-12-events.csv', 'c:\\\\Users\\\\n0170199\\\\Projects\\\\DE_Projects\\\\project_2_walkthrough/event_data\\\\2018-11-13-events.csv', 'c:\\\\Users\\\\n0170199\\\\Projects\\\\DE_Projects\\\\project_2_walkthrough/event_data\\\\2018-11-14-events.csv', 'c:\\\\Users\\\\n0170199\\\\Projects\\\\DE_Projects\\\\project_2_walkthrough/event_data\\\\2018-11-15-events.csv', 'c:\\\\Users\\\\n0170199\\\\Projects\\\\DE_Projects\\\\project_2_walkthrough/event_data\\\\2018-11-16-events.csv', 'c:\\\\Users\\\\n0170199\\\\Projects\\\\DE_Projects\\\\project_2_walkthrough/event_data\\\\2018-11-17-events.csv', 'c:\\\\Users\\\\n0170199\\\\Projects\\\\DE_Projects\\\\project_2_walkthrough/event_data\\\\2018-11-18-events.csv', 'c:\\\\Users\\\\n0170199\\\\Projects\\\\DE_Projects\\\\project_2_walkthrough/event_data\\\\2018-11-19-events.csv', 'c:\\\\Users\\\\n0170199\\\\Projects\\\\DE_Projects\\\\project_2_walkthrough/event_data\\\\2018-11-20-events.csv', 'c:\\\\Users\\\\n0170199\\\\Projects\\\\DE_Projects\\\\project_2_walkthrough/event_data\\\\2018-11-21-events.csv', 'c:\\\\Users\\\\n0170199\\\\Projects\\\\DE_Projects\\\\project_2_walkthrough/event_data\\\\2018-11-22-events.csv', 'c:\\\\Users\\\\n0170199\\\\Projects\\\\DE_Projects\\\\project_2_walkthrough/event_data\\\\2018-11-23-events.csv', 'c:\\\\Users\\\\n0170199\\\\Projects\\\\DE_Projects\\\\project_2_walkthrough/event_data\\\\2018-11-24-events.csv', 'c:\\\\Users\\\\n0170199\\\\Projects\\\\DE_Projects\\\\project_2_walkthrough/event_data\\\\2018-11-25-events.csv', 'c:\\\\Users\\\\n0170199\\\\Projects\\\\DE_Projects\\\\project_2_walkthrough/event_data\\\\2018-11-26-events.csv', 'c:\\\\Users\\\\n0170199\\\\Projects\\\\DE_Projects\\\\project_2_walkthrough/event_data\\\\2018-11-27-events.csv', 'c:\\\\Users\\\\n0170199\\\\Projects\\\\DE_Projects\\\\project_2_walkthrough/event_data\\\\2018-11-28-events.csv', 'c:\\\\Users\\\\n0170199\\\\Projects\\\\DE_Projects\\\\project_2_walkthrough/event_data\\\\2018-11-29-events.csv', 'c:\\\\Users\\\\n0170199\\\\Projects\\\\DE_Projects\\\\project_2_walkthrough/event_data\\\\2018-11-30-events.csv']\n"
     ]
    }
   ],
   "source": [
    "# Create a for loop to create a list of file paths\n",
    "# in `event_data` \n",
    "\n",
    "for root, dirs, files in os.walk(filepath):\n",
    "    # joining the file path of each file with the root directory\n",
    "    # to get the absolute path of the csv files within the \n",
    "    # `/home/workspace/event_data`\n",
    "    file_path_list = glob.glob(os.path.join(root,'*'))\n",
    "    print (file_path_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The above output shows us how `file_path_list` looks like. Let's see if we got all the file paths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "print(len(file_path_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now, we have the absolute file path of 30 files, one for each day in November 2018. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Let's process the files to create a new data file `.csv` that will contain records from all of the 30 files.  \n",
    "\n",
    "Don't worry too much how we have combined these `.csv` files into one file. Just know that we are going to use the combined file called `event_datafile_new.csv` to create tables in Apache Cassandra like we used `supermarket_sales.csv` in the last class. \n",
    "\n",
    "I've thoroughly commented the code here so that we don't have a problem understanding what it being done here.   \n",
    "\n",
    "The following code block: \n",
    "- Creates an empty list called `full_data_rows_list` \n",
    "- For each csv file in `file_path_list`: \n",
    "    - Skips the header row (which has the column names), \n",
    "    - Reads each row and \n",
    "    - Appends the row to the `full_data_rows_list` \n",
    "\n",
    "Then, once we have the list of rows from all `csv` files in `full_data_rows_list`, we create a new csv file called `event_datafile_new.csv`, write the header row (the column names) for the first row, and write each row from `full_data_rows_list` into the `event_datafile_new.csv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# initiating an empty list of rows which will be filled iteratively\n",
    "# by looping over each rows of each csv file\n",
    "full_data_rows_list = [] \n",
    "    \n",
    "# for every csv in the file_path_list \n",
    "for f in file_path_list:\n",
    "\n",
    "    # open the csv file as csvfile\n",
    "    with open(f, 'r', encoding = 'utf8', newline='') as csvfile: \n",
    "        # creating a csv reader object \n",
    "        csvreader = csv.reader(csvfile) \n",
    "        # skip the header \n",
    "        next(csvreader)\n",
    "        \n",
    "        # extract each data row one by one and append it to the\n",
    "        # full_data_rows_list \n",
    "        for line in csvreader:\n",
    "            full_data_rows_list.append(line) \n",
    "            \n",
    "# uncomment the code below if you would like to get total number of rows \n",
    "#print(len(full_data_rows_list))\n",
    "# uncomment the code below if you would like to check to see what the list of event data rows will look like\n",
    "#print(full_data_rows_list)\n",
    "\n",
    "# Creating a dialect (customising a csv parser) to \n",
    "# make sure all values are quoted and the initial space\n",
    "# after the delimiter is skipped. \n",
    "csv.register_dialect('myDialect', quoting=csv.QUOTE_ALL, \n",
    "                     skipinitialspace=True)\n",
    "\n",
    "# creating a event data csv file called event_datafile_new csv\n",
    "# that will be used to insert data into the Apache Cassandra\n",
    "# tables\n",
    "with open('event_datafile_new.csv', 'w', encoding = 'utf8', newline='') as f:\n",
    "    # create a writer with dialect created\n",
    "    writer = csv.writer(f, dialect='myDialect')\n",
    "    \n",
    "    # write the first row as the column names\n",
    "    writer.writerow(['artist','firstName','gender','itemInSession','lastName','length',\\\n",
    "                'level','location','sessionId','song','userId'])\n",
    "    \n",
    "    # for each row in the full_data_rows_list\n",
    "    for row in full_data_rows_list:\n",
    "        \n",
    "        if (row[0] == ''):\n",
    "            continue\n",
    "        # add data to the respective columns set\n",
    "        writer.writerow((row[0], row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[12], row[13], row[16]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Let's check the number of rows in this new csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6821\n"
     ]
    }
   ],
   "source": [
    "# check the number of rows in your csv file\n",
    "with open('event_datafile_new.csv', 'r', encoding = 'utf8') as f:\n",
    "    print(sum(1 for line in f)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Make sure that the number of rows you have is $6821$. If you get a different number of rows, your code may need to be `reset`.  \n",
    "\n",
    "<img src=\"images/reset.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "These are the first five rows of the `event_datafile_new.csv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>firstName</th>\n",
       "      <th>gender</th>\n",
       "      <th>itemInSession</th>\n",
       "      <th>lastName</th>\n",
       "      <th>length</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>song</th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Des'ree</td>\n",
       "      <td>Kaylee</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>Summers</td>\n",
       "      <td>246.30812</td>\n",
       "      <td>free</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "      <td>139</td>\n",
       "      <td>You Gotta Be</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mr Oizo</td>\n",
       "      <td>Kaylee</td>\n",
       "      <td>F</td>\n",
       "      <td>3</td>\n",
       "      <td>Summers</td>\n",
       "      <td>144.03873</td>\n",
       "      <td>free</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "      <td>139</td>\n",
       "      <td>Flat 55</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tamba Trio</td>\n",
       "      <td>Kaylee</td>\n",
       "      <td>F</td>\n",
       "      <td>4</td>\n",
       "      <td>Summers</td>\n",
       "      <td>177.18812</td>\n",
       "      <td>free</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "      <td>139</td>\n",
       "      <td>Quem Quiser Encontrar O Amor</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Mars Volta</td>\n",
       "      <td>Kaylee</td>\n",
       "      <td>F</td>\n",
       "      <td>5</td>\n",
       "      <td>Summers</td>\n",
       "      <td>380.42077</td>\n",
       "      <td>free</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "      <td>139</td>\n",
       "      <td>Eriatarka</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Infected Mushroom</td>\n",
       "      <td>Kaylee</td>\n",
       "      <td>F</td>\n",
       "      <td>6</td>\n",
       "      <td>Summers</td>\n",
       "      <td>440.26730</td>\n",
       "      <td>free</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "      <td>139</td>\n",
       "      <td>Becoming Insane</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              artist firstName gender  itemInSession lastName     length  \\\n",
       "0            Des'ree    Kaylee      F              1  Summers  246.30812   \n",
       "1            Mr Oizo    Kaylee      F              3  Summers  144.03873   \n",
       "2         Tamba Trio    Kaylee      F              4  Summers  177.18812   \n",
       "3     The Mars Volta    Kaylee      F              5  Summers  380.42077   \n",
       "4  Infected Mushroom    Kaylee      F              6  Summers  440.26730   \n",
       "\n",
       "  level                     location  sessionId                          song  \\\n",
       "0  free  Phoenix-Mesa-Scottsdale, AZ        139                  You Gotta Be   \n",
       "1  free  Phoenix-Mesa-Scottsdale, AZ        139                       Flat 55   \n",
       "2  free  Phoenix-Mesa-Scottsdale, AZ        139  Quem Quiser Encontrar O Amor   \n",
       "3  free  Phoenix-Mesa-Scottsdale, AZ        139                     Eriatarka   \n",
       "4  free  Phoenix-Mesa-Scottsdale, AZ        139               Becoming Insane   \n",
       "\n",
       "   userId  \n",
       "0       8  \n",
       "1       8  \n",
       "2       8  \n",
       "3       8  \n",
       "4       8  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('event_datafile_new.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We have the name of the `artist`, the `firstName` and `lastName` , the `gender` and `level` (free or paid), `location` and `userId` of the user, the name of the `song` and it's `length` and finally, the `sessionId`.  \n",
    "\n",
    "And here is the length of the dataframe: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6820, 11)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('event_datafile_new.csv').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "It's $6820$ as the first row is the header row. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Once you have the correct number of rows, you can proceed with Part II."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Part II. Complete the Apache Cassandra coding portion of your project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "__Now, you are ready to work with the CSV file titled `event_datafile_new.csv`, located within your Workspace directory. As a refresher, the `event_datafile_new.csv` contains the following columns__: \n",
    "\n",
    "- `artist`'s name \n",
    "- `firstName` of user\n",
    "- `gender` of user\n",
    "- `itemInSession`, which refers to the item number in session\n",
    "- `lastName` of user\n",
    "- `length` of the song \n",
    "- `level` of the user (paid or free)\n",
    "- `location` of the user\n",
    "- `sessionId` \n",
    "- `song` title \n",
    "- `userId` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Begin writing your Apache Cassandra Code in the cells below  \n",
    "\n",
    "### Creating a Cluster  \n",
    "\n",
    "The following code connect to our local instance of Apache Cassandra (if we have one). This connection will reach out to the database and ensure we have the correct privileges to connect to this database.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Make a connection to a Cassandra instance on\n",
    "# your local machine (127.0.0.1) \n",
    "from cassandra.cluster import Cluster\n",
    "\n",
    "try:\n",
    "    cluster = Cluster(['127.0.0.1'])\n",
    "    session = cluster.connect()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Once we create our cluster object, we need to connect to it. This will create our session that we will use to execute the queries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x211b86a7550>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Create a seession to establish connection and\n",
    "# begin executing queries   \n",
    "session.execute(\"\"\"\n",
    "     CREATE KEYSPACE IF NOT EXISTS events\n",
    "    WITH REPLICATION =\n",
    "    { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "This is very synonymous to what we used to do when we connected to the PostgreSQL database and got a cursor to it using code like this: \n",
    "\n",
    "```python \n",
    "conn = psycopg2.connect(\"host=127.0.0.1 dbname=studentdb user=student password=student\")\n",
    "cur = conn.cursor()\n",
    "```  \n",
    "\n",
    "The `session` variable will help us execute queries. This is similar to how we used the `cur` variable or cursor with `psycopg2`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Create Keyspace  \n",
    "\n",
    "A keyspace in Cassandra is synonymous to a database you would create in PostgreSQL. It defines the _replication strategy_ and the _replication factor_.  \n",
    "\n",
    "The _replication factor_ tells us how many copies of the data will be distributed across the nodes. The _replication strategy_ specifies _how_ the replication should take place. \n",
    "\n",
    "You can learn more about it [here](https://www.tutorialspoint.com/cassandra/cassandra_create_keyspace.htm). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7f240b0bac18>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Create a Keyspace  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "If you want to a refresher on how to create a keyspace, refer to the [Lesson 4: Excerise 3: Solution](https://classroom.udacity.com/nanodegrees/nd027/parts/f7dbb125-87a2-4369-bb64-dc5c21bb668a/modules/c0e48224-f2d0-4bf5-ac02-3e1493e530fc/lessons/73fd6e35-3319-4520-94b5-9651437235d7/concepts/512d3115-ba9c-41f7-a1fd-0a676292e7d5) in the classroom. \n",
    "\n",
    "Here, we are creating a database which will consists tables relating to the events/logs of users on a music app. So, you can name your keyspace accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Set Keyspace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Set Keyspace to the keyspace specified above \n",
    "session.set_keyspace('events') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### ✍️ Notes\n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Now, we need to create tables to run the following queries. Remember, with Apache Cassandra, you model the database tables on queries you want to run.  \n",
    "\n",
    "This is done whenever we are dealing with big data. If we think about the queries first, we can appropriately partition and sort our data while creating tables. This will make our reads super fast, as the data is already partitioned and sorted! \n",
    "\n",
    "## Create queries to ask the following three questions of the data  \n",
    "\n",
    "## 1. Give me the artist, song title and song's length in the music app history that was heard during `sessionId = 338` and `itemInSession = 4` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Before creating the table for this, let's first understand what our query is going to be. \n",
    "\n",
    "- It's going to be selecting the columns artist, song's title and song's length. \n",
    "- It's going to look for the records based on the value of `sessionId` and `itemInSession`. \n",
    "\n",
    "Think of these questions while creating the table: \n",
    "1. What columns should be in this table?  \n",
    "\n",
    "2. What datatype should each column be? \n",
    "> - [Here](https://www.guru99.com/cassandra-data-types-expiration-tutorial.html) are a list of datatypes you have in CQL. Make sure to have a look at this. Data types like `NUMERIC` are not available here. \n",
    "> - Some important ones which we use alot are `INT`, `TEXT`, `VARINT`, `FLOAT`, `BIGINT`,  `TIMESTAMP`, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "3. What should the [Primary Key](https://classroom.udacity.com/nanodegrees/nd027/parts/f7dbb125-87a2-4369-bb64-dc5c21bb668a/modules/c0e48224-f2d0-4bf5-ac02-3e1493e530fc/lessons/73fd6e35-3319-4520-94b5-9651437235d7/concepts/0600fb6e-935a-4b6b-abd2-16bff1016924) of the table be? Which should be our partition keys, if any? Which should be our [Clustering Columns](https://classroom.udacity.com/nanodegrees/nd027/parts/f7dbb125-87a2-4369-bb64-dc5c21bb668a/modules/c0e48224-f2d0-4bf5-ac02-3e1493e530fc/lessons/73fd6e35-3319-4520-94b5-9651437235d7/concepts/347092ad-2042-4385-90e5-b258f41941f4), if any?  \n",
    "\n",
    "Remember: \n",
    "- The Primary Key (simple or composite) of a row should be the unique identifier of the row. There should be no 2 rows with the same Primary Key. \n",
    "- Look at the `WHERE` of your query to understand your partition keys. Using which columns will you be filtering the records? \n",
    "- Look at how you want to order your results. These will help in deciding your clustering columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE IF NOT EXISTS music_history(artist TEXT,\n",
      "song_title TEXT,\n",
      "song_length FLOAT,\n",
      "session_id INT,\n",
      "iteminsession INT,\n",
      "PRIMARY KEY (session_id, iteminsession))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x211b6626ac0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO-DO: Query 1:  Give me the artist, song title and song's \n",
    "# length in the music app history that was heard during \n",
    "# sessionId = 338 and itemInSession = 4 \n",
    "\n",
    "query = \"CREATE TABLE IF NOT EXISTS music_history\"\n",
    "query = query + \"\"\"(artist TEXT,\n",
    "song_title TEXT,\n",
    "song_length FLOAT,\n",
    "session_id INT,\n",
    "iteminsession INT,\n",
    "PRIMARY KEY (session_id, iteminsession))\"\"\"\n",
    "\n",
    "print(query)\n",
    "session.execute(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Again, for a revision on how to create tables in apache cassandra, refer to the [Lesson 4: Excerise 3: Solution](https://classroom.udacity.com/nanodegrees/nd027/parts/f7dbb125-87a2-4369-bb64-dc5c21bb668a/modules/c0e48224-f2d0-4bf5-ac02-3e1493e530fc/lessons/73fd6e35-3319-4520-94b5-9651437235d7/concepts/512d3115-ba9c-41f7-a1fd-0a676292e7d5) in the classroom.   \n",
    "\n",
    "\n",
    "This is a screenshot from the solution notebook: \n",
    "<img src=\"images/create_table_example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Once we have created our table, we can start inserting data into it. \n",
    "\n",
    "The following code will loop through each row in the `event_datafile_new.csv` and insert the relevant data from each row into the table you just created. \n",
    "\n",
    "\n",
    "There are 2 `TO-DO`s here: \n",
    "- Write `INSERT` statement that will be used to insert the relevant data from each record into the table you just created. For a refresher on how to insert data into tables, refer to the [Lesson 4: Excerise 3: Solution](https://classroom.udacity.com/nanodegrees/nd027/parts/f7dbb125-87a2-4369-bb64-dc5c21bb668a/modules/c0e48224-f2d0-4bf5-ac02-3e1493e530fc/lessons/73fd6e35-3319-4520-94b5-9651437235d7/concepts/14222a9b-5d25-4b97-a09b-18d5f5a1cdc4) in the classroom.  \n",
    "\n",
    "This is how it looks in the classroom: \n",
    "<img src=\"images/insert_cassandra.png\">\n",
    "\n",
    "Now, as you notice, there are two parts to the `INSERT` query. The first is the `query` in the screenshot, and then there are the actual values that we need to put into the respective columns. Extracting these values from the current row is what the second TO-DO is all about. \n",
    "\n",
    "\n",
    "- Assign which values from the current row or `line` should be assigned for each column in the `INSERT` statement that you create. For example, the artist's name and the user's first name will be the first 2 values in each row or `line` in the `csv`. In order to insert the artist's name, you would use `line[0]` and in order to insert the user's first name, you would use `line[1]`.  \n",
    "\n",
    "__Note:__ All values in the current `csv` are of type `str`, you might want to do something about them when inserting these values in the table. You cannot insert a string wherein an integer or float is expected and expect it to be automatically be converted in the table. \n",
    "\n",
    "In order for you to understand which column's value comes at what index, I have written this code which you can use: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 artist <class 'str'>\n",
      "1 firstName <class 'str'>\n",
      "2 gender <class 'str'>\n",
      "3 itemInSession <class 'str'>\n",
      "4 lastName <class 'str'>\n",
      "5 length <class 'str'>\n",
      "6 level <class 'str'>\n",
      "7 location <class 'str'>\n",
      "8 sessionId <class 'str'>\n",
      "9 song <class 'str'>\n",
      "10 userId <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    for line in csvreader:\n",
    "        for i, value in enumerate(line):\n",
    "            print( i, value,type(value))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We have provided part of the code to set up the CSV file. Please complete the \n",
    "# Apache Cassandra code below \n",
    "\n",
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    # skip header line\n",
    "    next(csvreader) \n",
    "    for line in csvreader:\n",
    "        ## TODO: Assign the INSERT Statement\n",
    "        query = \"\"\"INSERT INTO music_history \n",
    "                    (artist, song_title, song_length, session_id, iteminsession)\"\"\"\n",
    "        # TODO: Assign the Placeholder Values\n",
    "        query = query + \"\"\"VALUES (%s, %s, %s, %s,%s)\"\"\"\n",
    "        ## TO-DO: Assign which column element should be assigned for each column in the INSERT statement.\n",
    "        ## For e.g., to INSERT artist_name and user first_name, you would change the code below to `line[0], line[1]`\n",
    "        session.execute(query, (line[0], \n",
    "                                line[9],\n",
    "                                float(line[5]),\n",
    "                                int(line[8]),\n",
    "                                int(line[3])\n",
    "                                ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Do a `SELECT` to verify that the data have been inserted into each table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithless Music Matters (Mark Knight Dub) 495.30731201171875\n"
     ]
    }
   ],
   "source": [
    "# TO-DO: Add in the SELECT statement to verify the data was \n",
    "# entered into the table correctly  \n",
    "query = \"\"\"select artist, song_title, song_length\n",
    "from music_history\n",
    "WHERE session_id = 338 and iteminsession = 4 \"\"\"\n",
    "\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e: \n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print(row.artist, row.song_title, row.song_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 2. Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name) for userid = 10, sessionid = 182\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Again, you will be following the same process here. \n",
    "\n",
    "Let's understand what our query is going to be: \n",
    "1. We want the artist name, and user's first and last name\n",
    "2. It's going to look for records based on value of `userId` and `sessionId`.  \n",
    "3. The table is going to be sorted by `itemInSession` \n",
    "\n",
    "Think of these questions when creating the table:  \n",
    "\n",
    "1. What columns should be in this table?  \n",
    "\n",
    "2. What datatype should each column be? \n",
    "> - [Here](https://www.guru99.com/cassandra-data-types-expiration-tutorial.html) are a list of datatypes you have in CQL. Make sure to have a look at this. Data types like `NUMERIC` (which were available in PostgreSQL are not available here. \n",
    "> - Some important ones which we use alot are `INT`, `TEXT`, `VARINT`, `FLOAT`, `BIGINT`,  `TIMESTAMP`, etc.\n",
    "3. What should the [Primary Key](https://classroom.udacity.com/nanodegrees/nd027/parts/f7dbb125-87a2-4369-bb64-dc5c21bb668a/modules/c0e48224-f2d0-4bf5-ac02-3e1493e530fc/lessons/73fd6e35-3319-4520-94b5-9651437235d7/concepts/0600fb6e-935a-4b6b-abd2-16bff1016924) of the table be? Which should be our partition keys, if any? Which should be our [Clustering Columns](https://classroom.udacity.com/nanodegrees/nd027/parts/f7dbb125-87a2-4369-bb64-dc5c21bb668a/modules/c0e48224-f2d0-4bf5-ac02-3e1493e530fc/lessons/73fd6e35-3319-4520-94b5-9651437235d7/concepts/347092ad-2042-4385-90e5-b258f41941f4), if any? \n",
    "\n",
    "Remember: \n",
    "- The Primary Key (simple or composite) of a row should be the unique identifier of the row. There should be no 2 rows with the same Primary Key. \n",
    "- Look at the `WHERE` of your query to understand your partition keys. Using which columns will you be filtering the records? \n",
    "- Look at how you want to order your results. These will help in figuring out your clustering columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE IF NOT EXISTS table2(\n",
      "                    artist TEXT,\n",
      "                    song TEXT,\n",
      "                    user TEXT,\n",
      "                    sessionid INT,\n",
      "                    userid INT,\n",
      "                    iteminsession INT,\n",
      "                    PRIMARY KEY ((userid, sessionid),iteminsession))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x211b877c9a0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO-DO: Query 2:  Give me only the following: name of artist, \n",
    "# song (sorted by itemInSession) and user (first and last name)\n",
    "# for userid = 10, sessionid = 182 \n",
    "query = \"CREATE TABLE IF NOT EXISTS table2\"\n",
    "query = query + \"\"\"(\n",
    "                    artist TEXT,\n",
    "                    song TEXT,\n",
    "                    user TEXT,\n",
    "                    sessionid INT,\n",
    "                    userid INT,\n",
    "                    iteminsession INT,\n",
    "                    PRIMARY KEY ((userid, sessionid),iteminsession))\"\"\"\n",
    "\n",
    "print(query)\n",
    "session.execute(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Again, for a revision on how to create tables in apache cassandra, refer to the [Lesson 4: Excerise 3: Solution](https://classroom.udacity.com/nanodegrees/nd027/parts/f7dbb125-87a2-4369-bb64-dc5c21bb668a/modules/c0e48224-f2d0-4bf5-ac02-3e1493e530fc/lessons/73fd6e35-3319-4520-94b5-9651437235d7/concepts/512d3115-ba9c-41f7-a1fd-0a676292e7d5) in the classroom.   \n",
    "\n",
    "\n",
    "This is how one of them looks like: \n",
    "<img src=\"images/create_table_example.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Once we have created our table, we can start inserting data into it. \n",
    "\n",
    "The following code will loop through each row in the `event_datafile_new.csv` and insert the relevant data from each row into the table you just created. \n",
    "\n",
    "\n",
    "There are 2 `TO-DO`s here: \n",
    "- Write `INSERT` statement that will be used to insert the relevant data from each record into the table you just created. For a refresher on how to insert data into tables, refer to the [Lesson 4: Excerise 3: Solution](https://classroom.udacity.com/nanodegrees/nd027/parts/f7dbb125-87a2-4369-bb64-dc5c21bb668a/modules/c0e48224-f2d0-4bf5-ac02-3e1493e530fc/lessons/73fd6e35-3319-4520-94b5-9651437235d7/concepts/14222a9b-5d25-4b97-a09b-18d5f5a1cdc4) in the classroom.  \n",
    "\n",
    "This is how it looks in the classroom: \n",
    "<img src=\"images/insert_cassandra.png\">\n",
    "\n",
    "Now, as you notice, there are two parts to the `INSERT` query. The first is the `query` in the screenshot, and then there are the actual values that we need to put into the respective columns. Extracting these values from the current row is what the second TO-DO is all about. \n",
    "\n",
    "\n",
    "- Assign which values from the current row or `line` should be assigned for each column in the `INSERT` statement that you create. For example, the artist's name and the user's first name will be the first 2 values in each row or `line` in the `csv`. In order to insert the artist's name, you would use `line[0]` and in order to insert the user's first name, you would use `line[1]`.  \n",
    "\n",
    "__Note:__ All values in the current `csv` are of type `str`, you might want to do something about them when inserting these values in the table. You cannot insert a string wherein an integer or float is expected and expect it to be automatically be converted in the table. \n",
    "\n",
    "In order for you to understand which column's value comes at what index, I have written this code which you can use: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 artist <class 'str'>\n",
      "1 firstName <class 'str'>\n",
      "2 gender <class 'str'>\n",
      "3 itemInSession <class 'str'>\n",
      "4 lastName <class 'str'>\n",
      "5 length <class 'str'>\n",
      "6 level <class 'str'>\n",
      "7 location <class 'str'>\n",
      "8 sessionId <class 'str'>\n",
      "9 song <class 'str'>\n",
      "10 userId <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    for line in csvreader:\n",
    "        for i, value in enumerate(line):\n",
    "            print( i, value,type(value))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We have provided part of the code to set up the CSV file. Please complete the Apache Cassandra code below#\n",
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    # skip header line\n",
    "    next(csvreader) \n",
    "    for line in csvreader:\n",
    "## TO-DO: Assign the INSERT statements into the `query` variable\n",
    "        query = \"INSERT INTO table2 (artist, song, user, sessionid, userid, iteminsession)\"\n",
    "        query = query + \"VALUES (%s,%s,%s,%s,%s,%s)\"\n",
    "        ## TO-DO: Assign which column element should be assigned for each column in the INSERT statement.\n",
    "        ## For e.g., to INSERT artist_name and user first_name, you would change the code below to `line[0], line[1]`\n",
    "        session.execute(query, (line[0], \n",
    "                                line[9],\n",
    "                                line[1] + ' ' + line[4],\n",
    "                                int(line[8]),\n",
    "                                int(line[10]),\n",
    "                                int(line[3]),\n",
    "                                ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Do a `SELECT` to verify that the data have been inserted into each table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down To The Bone Keep On Keepin' On Sylvie Cruz\n",
      "Three Drives Greece 2000 Sylvie Cruz\n",
      "Sebastien Tellier Kilometer Sylvie Cruz\n",
      "Lonnie Gordon Catch You Baby (Steve Pitron & Max Sanna Radio Edit) Sylvie Cruz\n"
     ]
    }
   ],
   "source": [
    "# TO-DO: Add in the SELECT statement to verify the data was \n",
    "# entered into the table correctly   \n",
    "query = \"\"\"select artist, song, user\n",
    "from table2\n",
    "WHERE userid = 10 and sessionid = 182 \"\"\"\n",
    "\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e: \n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print(row.artist, row.song, row.user)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 3. Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Again, you will be following the same process here. \n",
    "\n",
    "Let's understand what our query is going to be: \n",
    "1. We want the first and last name of the user \n",
    "2. It's going to look for records the name of the song\n",
    "\n",
    "Think of these questions when creating the table:  \n",
    "\n",
    "1. What columns should be in this table?  \n",
    "\n",
    "2. What datatype should each column be? \n",
    "> - [Here](https://www.guru99.com/cassandra-data-types-expiration-tutorial.html) are a list of datatypes you have in CQL. Make sure to have a look at this. Data types like `NUMERIC` (which were available in PostgreSQL are not available here. \n",
    "> - Some important ones which we use alot are `INT`, `TEXT`, `VARINT`, `FLOAT`, `BIGINT`,  `TIMESTAMP`, etc.\n",
    "3. What should the [Primary Key](https://classroom.udacity.com/nanodegrees/nd027/parts/f7dbb125-87a2-4369-bb64-dc5c21bb668a/modules/c0e48224-f2d0-4bf5-ac02-3e1493e530fc/lessons/73fd6e35-3319-4520-94b5-9651437235d7/concepts/0600fb6e-935a-4b6b-abd2-16bff1016924) of the table be? Which should be our partition keys, if any? Which should be our [Clustering Columns](https://classroom.udacity.com/nanodegrees/nd027/parts/f7dbb125-87a2-4369-bb64-dc5c21bb668a/modules/c0e48224-f2d0-4bf5-ac02-3e1493e530fc/lessons/73fd6e35-3319-4520-94b5-9651437235d7/concepts/347092ad-2042-4385-90e5-b258f41941f4), if any? \n",
    "\n",
    "Remember: \n",
    "- The Primary Key (simple or composite) of a row should be the unique identifier of the row. There should be no 2 rows with the same Primary Key. \n",
    "- Look at the `WHERE` of your query to understand your partition keys. Using which columns will you be filtering the records? \n",
    "- Look at how you want to order your results. These will help in figuring out your clustering columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE IF NOT EXISTS table3(\n",
      "                    song TEXT,\n",
      "                    user TEXT,\n",
      "                    PRIMARY KEY ( song, user ))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x211b8790a90>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO-DO: Query 3:  Give me every user name (first and last) in my music app history who \n",
    "# listened to the song 'All Hands Against His Own' \n",
    "query = \"CREATE TABLE IF NOT EXISTS table3\"\n",
    "query = query + \"\"\"(\n",
    "                    song TEXT,\n",
    "                    user TEXT,\n",
    "                    PRIMARY KEY ( song, user ))\"\"\"\n",
    "\n",
    "print(query)\n",
    "session.execute(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Again, for a revision on how to create tables in apache cassandra, refer to the [Lesson 4: Excerise 3: Solution](https://classroom.udacity.com/nanodegrees/nd027/parts/f7dbb125-87a2-4369-bb64-dc5c21bb668a/modules/c0e48224-f2d0-4bf5-ac02-3e1493e530fc/lessons/73fd6e35-3319-4520-94b5-9651437235d7/concepts/512d3115-ba9c-41f7-a1fd-0a676292e7d5) in the classroom.   \n",
    "\n",
    "\n",
    "This is how one of them looks like: \n",
    "<img src=\"images/create_table_example.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Once we have created our table, we can start inserting data into it. \n",
    "\n",
    "The following code will loop through each row in the `event_datafile_new.csv` and insert the relevant data from each row into the table you just created. \n",
    "\n",
    "\n",
    "There are 2 `TO-DO`s here: \n",
    "- Write `INSERT` statement that will be used to insert the relevant data from each record into the table you just created. For a refresher on how to insert data into tables, refer to the [Lesson 4: Excerise 3: Solution](https://classroom.udacity.com/nanodegrees/nd027/parts/f7dbb125-87a2-4369-bb64-dc5c21bb668a/modules/c0e48224-f2d0-4bf5-ac02-3e1493e530fc/lessons/73fd6e35-3319-4520-94b5-9651437235d7/concepts/14222a9b-5d25-4b97-a09b-18d5f5a1cdc4) in the classroom.  \n",
    "\n",
    "This is how it looks in the classroom: \n",
    "<img src=\"images/insert_cassandra.png\">\n",
    "\n",
    "Now, as you notice, there are two parts to the `INSERT` query. The first is the `query` in the screenshot, and then there are the actual values that we need to put into the respective columns. Extracting these values from the current row is what the second TO-DO is all about. \n",
    "\n",
    "\n",
    "- Assign which values from the current row or `line` should be assigned for each column in the `INSERT` statement that you create. For example, the artist's name and the user's first name will be the first 2 values in each row or `line` in the `csv`. In order to insert the artist's name, you would use `line[0]` and in order to insert the user's first name, you would use `line[1]`.  \n",
    "\n",
    "__Note:__ All values in the current `csv` are of type `str`, you might want to do something about them when inserting these values in the table. You cannot insert a string wherein an integer or float is expected and expect it to be automatically be converted in the table. \n",
    "\n",
    "In order for you to understand which column's value comes at what index, I have written this code which you can use: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 artist <class 'str'>\n",
      "1 firstName <class 'str'>\n",
      "2 gender <class 'str'>\n",
      "3 itemInSession <class 'str'>\n",
      "4 lastName <class 'str'>\n",
      "5 length <class 'str'>\n",
      "6 level <class 'str'>\n",
      "7 location <class 'str'>\n",
      "8 sessionId <class 'str'>\n",
      "9 song <class 'str'>\n",
      "10 userId <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    for line in csvreader:\n",
    "        for i, value in enumerate(line):\n",
    "            print( i, value,type(value))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We have provided part of the code to set up the CSV file. Please complete the Apache Cassandra code below#\n",
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    # skip header line\n",
    "    next(csvreader) \n",
    "    for line in csvreader:\n",
    "## TO-DO: Assign the INSERT statements into the `query` variable\n",
    "        query = \"INSERT INTO table3 (song, user)\"\n",
    "        query = query + \"VALUES (%s,%s)\"\n",
    "        ## TO-DO: Assign which column element should be assigned for each column in the INSERT statement.\n",
    "        ## For e.g., to INSERT artist_name and user first_name, you would change the code below to `line[0], line[1]`\n",
    "        session.execute(query, (line[9], line[1] + ' ' + line[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Do a `SELECT` to verify that the data have been inserted into each table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacqueline Lynch\n",
      "Sara Johnson\n",
      "Tegan Levine\n"
     ]
    }
   ],
   "source": [
    "# TO-DO: Add in the SELECT statement to verify the data was \n",
    "# entered into the table correctly   \n",
    "query = \"\"\"select user\n",
    "from table3\n",
    "WHERE song = 'All Hands Against His Own' \"\"\"\n",
    "\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e: \n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print(row.user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "If you did everything right, you should see something like this in the output: \n",
    "\n",
    "<img src=\"images/answer_3.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Drop the tables before closing out the sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T21:32:15.075846Z",
     "start_time": "2019-05-03T21:32:15.065500Z"
    },
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x211b8d1d340>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TO-DO: Drop the tables before closing out the sessions \n",
    "session.execute(\"DROP TABLE music_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x211b8bc6250>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.execute(\"DROP TABLE table2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x211b881fe80>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.execute(\"DROP TABLE table3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Again, for revising how to drop each table you created, you can refer to [Lesson 4: Excerise 3: Solution](https://classroom.udacity.com/nanodegrees/nd027/parts/f7dbb125-87a2-4369-bb64-dc5c21bb668a/modules/c0e48224-f2d0-4bf5-ac02-3e1493e530fc/lessons/73fd6e35-3319-4520-94b5-9651437235d7/concepts/14222a9b-5d25-4b97-a09b-18d5f5a1cdc4) in the classroom.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Close the session and cluster connection¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Notice how this action is similar to how we used to close the connection with the postgresql database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<a id=\"reflection\"></a>\n",
    "## Reflection \n",
    "> #### [Tweet] Your Learnings! \n",
    "> ###  I used to think ______, now I think ___. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Before submitting the project, make sure your project abides by  the [Project Rubric](https://review.udacity.com/#!/rubrics/2475/view)\n",
    "\n",
    "\n",
    "### ETL Pipeline Processing \n",
    "\n",
    "✅  Student creates `event_data_new.csv` file.    \n",
    "\n",
    "✅  Student uses the appropriate datatype within the CREATE statement.For e.g., `artist_name` and `song_title` use `TEXT`, length use `FLOAT` datatypes.  \n",
    "\n",
    "#### Data Modeling \n",
    "\n",
    "✅  Student creates the correct Apache Cassandra tables for each of the three queries. The `CREATE TABLE` statement should include the appropriate table. Student should adhere to the one table per query rule of Apache Cassandra. The student is allowed to use the same table for two of the questions, where it makes sense.    \n",
    "\n",
    "\n",
    "✅  Student demonstrates good understanding of data modeling by generating correct `SELECT` statements to generate the result being asked for in the question. The SELECT statement should NOT use `ALLOW FILTERING` to generate the results. For e.g., Query 3, `SELECT `statement should not require anything more than user name first and last name in the `SELECT` statement IF the table has been created with the correct COMPOSITE PRIMARY KEY, including partitions and clustering columns.   \n",
    "\n",
    "\n",
    "✅  Student should use table names that reflect the query and the result it will generate. Table names should include alphanumeric characters and underscores, and table names must start with a letter.We are looking for table names that provide a good general sense of what this query will generate. For e.g., for Query 2, an appropriate table name should reflect song playlist in session (e.g., name could be `song_playlist_session`). Students should not be using table names like `query_1` or `project_1`, etc. as table names need to be descriptive. \n",
    "\n",
    "\n",
    "✅  The sequence in which columns appear should reflect how the data is partitioned and the order of the data within the partitions. The sequence of the columns in the `CREATE` and `INSERT` statements should follow the order of the COMPOSITE PRIMARY KEY and CLUSTERING columns. The data should be inserted and retrieved in the same order as how the COMPOSITE PRIMARY KEY is set up. This is important to the student because Apache Cassandra is a partition row store, which means the partition key determines which any particular row is stored on which node. In case of composite partition key, partitions are distributed across the nodes of the cluster and how they are chunked for write purposes. Any clustering column(s) would determine the order in which the data is sorted within the partition.   \n",
    "\n",
    "#### PRIMARY KEYS\n",
    "\n",
    "✅  The combination of the PARTITION KEY alone or with the addition of CLUSTERING COLUMNS should be used appropriately to uniquely identify each row.For e.g., in Query 3, student should not only use song as PARTITION KEY. Similarly, the student does not need to user both `firstName` and `lastName` along with `userId` for query 3 clustering columns, as `song` and `userId` together will uniquely identify each row. The student should include clustering columns as part of the COMPOSITE PRIMARY KEY and understand that a COMPOSITE PRIMARY KEY uniquely identifies each row.   \n",
    "\n",
    "#### Presentation \n",
    "\n",
    "✅  The notebooks should include a description of the query the data is modeled after. The student can include headers right above the SELECT statement cell to highlight the responses to the questions.  \n",
    "\n",
    "\n",
    "✅  Code should be organized well into the different queries. Any in-line comments that were clearly part of the project instructions should be removed so the notebook provides a professional look.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
